\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Adversarial Attacks Against Mathematical Content Recognition in Vision-Language Models: Development, Evaluation, and Physical Testing}

\author{\IEEEauthorblockN{Anonymous}
\IEEEauthorblockA{Department of Computer Science\\
University, Country\\
email@university.edu}}

\maketitle

\begin{abstract}
Vision-Language Models (VLMs) have achieved remarkable capabilities in interpreting and solving complex mathematical content from images. This paper presents a systematic approach to developing and evaluating adversarial attacks against these models, specifically targeting their ability to process mathematical documents. We introduce a comprehensive framework for developing LaTeX-based attacks, a rigorous testing methodology using three distinct task types, and metrics for quantifying attack effectiveness. Our evaluation demonstrates that certain combinations of subtle visual manipulations can significantly reduce VLMs' performance on mathematical reasoning tasks while maintaining human readability. We further validate our findings through physical testing, examining how the printing and re-digitization process affects attack efficacy. The results provide insights into the robustness of VLMs and offer practical methods for protecting sensitive mathematical content from unauthorized AI processing.
\end{abstract}

\begin{IEEEkeywords}
adversarial attacks, vision-language models, document security, mathematical content, AI safety
\end{IEEEkeywords}

\section{Introduction}
The rapid advancement of Vision-Language Models (VLMs) has led to powerful systems capable of interpreting and solving complex mathematical problems directly from images. While these capabilities offer significant benefits for applications like educational tools and accessibility features, they also raise concerns about the unauthorized processing of sensitive mathematical content, particularly in academic and assessment contexts.

This paper presents a systematic investigation of adversarial attacks specifically designed to protect mathematical content in documents from VLM interpretation while maintaining human readability. Unlike traditional adversarial attacks that focus on image classification tasks, our approach targets the complex reasoning processes involved in mathematical content recognition and solution.

Our contributions include:
\begin{itemize}
    \item A comprehensive framework for developing LaTeX-based adversarial attacks targeting mathematical content recognition
    \item A rigorous testing methodology using multiple VLMs and three distinct task types
    \item Novel metrics for quantifying attack effectiveness based on output reduction
    \item Empirical evaluation of various attack types, highlighting the most effective strategies
    \item Physical testing validation to assess real-world applicability
\end{itemize}

\section{Related Work}
Prior work on adversarial attacks has primarily focused on image classification tasks \cite{example1, example2}, with limited exploration of attacks against VLMs processing structured content like mathematics. Research on document security has explored watermarking \cite{example3} and physical unclonable functions \cite{example4}, but these approaches often prioritize document authentication rather than content protection from AI systems.

Recent studies have begun exploring the robustness of VLMs to visual perturbations \cite{example5, example6}, but lack a systematic framework for development and evaluation specifically tailored to mathematical content.

\section{Attack Development Framework}
\subsection{Attack Design Philosophy}
Our attack development framework is guided by three key principles:
\begin{itemize}
    \item \textbf{Human Readability}: Attacks should not significantly impact human ability to read and understand the content
    \item \textbf{Mathematical Focus}: Attacks should specifically target mathematical notation, symbols, and reasoning structures
    \item \textbf{Systematic Variation}: Attacks should be parameterizable to enable controlled experiments
\end{itemize}

\subsection{LaTeX Implementation Pipeline}
We implemented attacks through a custom LaTeX preprocessing pipeline that generates document variants with precisely controlled modifications. Our system uses \texttt{exam\_attack\_v3.py} to transform a baseline LaTeX template into attack variants by injecting attack-specific LaTeX commands.

The pipeline consists of the following steps:
\begin{enumerate}
    \item Parse attack configuration (attack type and parameters)
    \item Load and parse the template LaTeX document
    \item Apply the specified attack transformations to the document structure
    \item Generate the modified LaTeX file
    \item Compile to PDF using pdflatex
\end{enumerate}

\subsection{Attack Taxonomy}
We developed and tested attacks across several categories:

\subsubsection{Visual Interference Attacks}
\begin{itemize}
    \item \textbf{Watermark Tiling}: Overlaying repeated text or mathematical symbols with controlled opacity, density, and angle
    \item \textbf{Texture Patterns}: Applying background patterns like waves, grids, or dots with parameterized density and contrast
\end{itemize}

\subsubsection{Mathematical Symbol Manipulation}
\begin{itemize}
    \item \textbf{Symbol Stretching}: Modifying the aspect ratio of specific mathematical operators (e.g., stretching integral or summation symbols)
    \item \textbf{Math-Specific Font Substitutions}: Replacing standard mathematical symbols with alternative font styles (e.g., blackboard, fraktur, or calligraphic variants)
\end{itemize}

\subsubsection{Layout and Spacing Attacks}
\begin{itemize}
    \item \textbf{Targeted Kerning}: Adjusting the spacing between specific characters or symbols
    \item \textbf{Custom Environment Targeting}: Modifying specific LaTeX environments (e.g., equation, align) with borders, backgrounds, or altered spacing
\end{itemize}

\subsubsection{Character-Level Attacks}
\begin{itemize}
    \item \textbf{Homoglyph Substitution}: Replacing characters with visually similar alternatives
    \item \textbf{Ligature Disruption}: Breaking standard character combinations
    \item \textbf{Low-Contrast Text}: Reducing the contrast of specific terms
\end{itemize}

\subsubsection{Combination Attacks}
\begin{itemize}
    \item \textbf{Triple Threat}: Combining watermarking, kerning, and symbol stretching
    \item \textbf{Kitchen Sink}: Combining watermarking, texture patterns, kerning, and symbol stretching
\end{itemize}

\section{Testing Methodology}
\subsection{Test Framework}
We developed a comprehensive testing framework (\texttt{run\_experiment\_v3.py}) that:
\begin{enumerate}
    \item Generates document variants using the attack pipeline
    \item Tests each variant against target VLMs using controlled prompts
    \item Logs detailed test results for analysis
    \item Analyzes attack effectiveness using multiple metrics
\end{enumerate}

\subsection{Target Models}
We evaluated attacks against multiple state-of-the-art VLMs:
\begin{itemize}
    \item Gemini Pro (gemma3:4b)
    \item Claude 3.5 Sonnet (claude-3.5-sonnet)
\end{itemize}

\subsection{Prompt Types}
Each document variant was tested using three distinct prompt types to evaluate different aspects of VLM capability:
\begin{itemize}
    \item \textbf{Transcription}: "Transcribe the text from the document."
    \item \textbf{Solving}: "Identify the main math or coding problem in this document and solve it. Show your work and explain your solution."
    \item \textbf{Explanation}: "Explain the concept being tested in this document and provide a detailed explanation of the problem and solution."
\end{itemize}

These prompt types represent increasing levels of reasoning complexity, allowing us to assess how attacks affect different cognitive tasks.

\subsection{Experimental Protocol}
Our experimental protocol followed these steps:

\begin{enumerate}
    \item Generate a baseline document without attacks
    \item Generate attack variants using systematically varied parameters
    \item Test the baseline and all variants against each model using all prompt types
    \item Record model outputs and measure effectiveness metrics
    \item Analyze results to identify the most effective attacks
    \item Conduct additional experiments with optimized parameters for top-performing attacks
\end{itemize}

For comprehensive evaluation, we conducted both targeted experiments with specific attack types and large-scale overnight experiments covering all attack variations.

\section{Evaluation Metrics}
\subsection{Primary Effectiveness Metric}
Our primary metric for attack effectiveness is the percentage reduction in output length relative to the baseline:

\begin{equation}
\text{Effectiveness} = \frac{\text{BaselineLength} - \text{AttackOutputLength}}{\text{BaselineLength}} \times 100\%
\end{equation}

This metric provides an intuitive measure of how effectively an attack disrupts the model's ability to process and respond to the content. Positive values indicate successful attacks (reduced output), while negative values indicate counterproductive attacks (increased output).

\subsection{Additional Metrics}
We also tracked:
\begin{itemize}
    \item \textbf{Absolute Output Length}: Raw character count of model outputs
    \item \textbf{Technical Failures}: Cases where the model returned an error message
    \item \textbf{Prompt-Specific Effectiveness}: Effectiveness broken down by prompt type
    \item \textbf{Model-Specific Effectiveness}: Effectiveness broken down by model
\end{itemize}

\subsection{Analysis Methods}
We employed several analysis approaches:
\begin{itemize}
    \item \textbf{Attack Type Aggregation}: Averaging effectiveness across all instances of each attack type
    \item \textbf{Individual Attack Ranking}: Identifying specific attack configurations with highest effectiveness
    \item \textbf{Prompt Type Analysis}: Comparing effectiveness across different prompt types
    \item \textbf{Model Comparison}: Analyzing differences in vulnerability between models
\end{itemize}

\section{Results and Discussion}
\subsection{Overall Attack Effectiveness}
Our experiments revealed that most attacks had mixed effectiveness, with many actually increasing model output length rather than reducing it. However, several attack types and specific configurations showed promising results.

The most effective attacks included:
\begin{itemize}
    \item \textbf{C4\_combo\_triple\_threat}: 47.72\% effectiveness (gemma3:4b, solving)
    \item \textbf{B1\_combo\_kerning\_and\_dense\_lines}: 40.55\% effectiveness (claude-3.5-sonnet, solving)
    \item \textbf{A1c\_watermark\_subtle\_symbols}: 35.53\% effectiveness (gemma3:4b, solving)
    \item \textbf{S2\_stretch\_sum}: 32.46\% effectiveness (gemma3:4b, explanation)
    \item \textbf{M3\_fraktur\_operators}: 30.13\% effectiveness (gemma3:4b, explanation)
\end{itemize}

\subsection{Task-Specific Effectiveness}
We observed significant variation in attack effectiveness across different prompt types:

\begin{itemize}
    \item \textbf{Solving Tasks}: Most vulnerable to attacks, with several attacks achieving >30\% effectiveness
    \item \textbf{Explanation Tasks}: Moderately vulnerable, particularly to mathematical symbol manipulation attacks
    \item \textbf{Transcription Tasks}: Least vulnerable, with most attacks showing negative effectiveness
\end{itemize}

This suggests that higher-level reasoning tasks are more susceptible to visual perturbations than basic text recognition tasks.

\subsection{Model-Specific Vulnerabilities}
We observed notable differences in vulnerability between models:

\begin{itemize}
    \item \textbf{Gemini Pro (gemma3:4b)}: More susceptible to mathematical symbol manipulation and combination attacks
    \item \textbf{Claude 3.5 Sonnet (claude-3.5-sonnet)}: More robust overall, but vulnerable to specific layout attacks
\end{itemize}

\subsection{Attack Type Analysis}
Analyzing effectiveness by attack type revealed:

\begin{itemize}
    \item \textbf{Combination Attacks}: Highest overall effectiveness, suggesting synergistic effects
    \item \textbf{Symbol Stretching}: Most effective single-technique attack, particularly targeting summation symbols
    \item \textbf{Math Font Manipulation}: Effective for explanation tasks, with fraktur operators showing strong results
    \item \textbf{Watermarking}: Mixed results, with subtle mathematical symbols performing better than text
    \item \textbf{Texture Patterns}: Generally ineffective, with some optimized variants showing modest results
\end{itemize}

\section{Physical Testing}
\subsection{Methodology}
To validate our findings in real-world scenarios, we conducted physical testing using the following protocol:

\begin{enumerate}
    \item Generated PDFs for the top-performing attacks
    \item Printed the documents on standard office paper
    \item Photographed the printed pages under normal office lighting
    \item Tested the photographs against the same VLMs using identical prompts
    \item Compared effectiveness metrics between digital and physical tests
\end{enumerate}

\subsection{Physical Testing Results}
The physical testing process revealed several important findings:

\begin{itemize}
    \item \textbf{Printing and Photography Effects}: The physical process generally amplified attack effectiveness due to added noise and reduced clarity
    \item \textbf{Attack Robustness}: Certain attacks (particularly symbol stretching and font substitution) maintained effectiveness through the physical process
    \item \textbf{Watermark Degradation}: Subtle watermarks often became less effective after printing due to reduced contrast
    \item \textbf{Combination Attack Stability}: Multi-technique attacks showed the most consistent effectiveness across digital and physical testing
\end{itemize}

\section{Conclusion}
Our research demonstrates that targeted adversarial attacks can significantly reduce VLMs' ability to process mathematical content while maintaining human readability. The most effective approaches combine multiple techniques, with combination attacks achieving up to 47.7\% effectiveness in reducing model output.

Key findings include:
\begin{itemize}
    \item Higher-level reasoning tasks (solving, explanation) are more vulnerable than basic transcription
    \item Mathematical symbol manipulation is particularly effective, especially when targeting common operators
    \item Models show different vulnerability patterns, suggesting the value of model-specific optimized attacks
    \item Physical testing confirms the real-world applicability of digital attacks with some modifications
\end{itemize}

These results provide a foundation for developing practical document protection systems that can selectively limit AI processing while maintaining human usability.

\subsection{Future Work}
Future research directions include:
\begin{itemize}
    \item Developing adaptive attacks that automatically customize parameters based on document content
    \item Exploring semantic effectiveness metrics beyond output length
    \item Investigating the transferability of attacks across a wider range of VLMs
    \item Developing robust defenses against these attacks
\end{itemize}

\begin{thebibliography}{00}
\bibitem{example1} A. Author, ``Example paper on adversarial attacks,'' in Proc. Conference on Computer Vision, 2023, pp. 1--8.
\bibitem{example2} B. Researcher, ``Survey of adversarial techniques,'' Journal of Machine Learning, vol. 10, no. 2, pp. 123--145, 2022.
\bibitem{example3} C. Scholar, ``Digital watermarking methods for document security,'' in Proc. Information Security Conference, 2023, pp. 45--52.
\bibitem{example4} D. Expert, ``Physical unclonable functions for document authentication,'' Transactions on Security, vol. 5, no. 3, pp. 234--251, 2022.
\bibitem{example5} E. Investigator, ``Robustness of vision-language models to input perturbations,'' ArXiv preprint arXiv:2304.XXXXX, 2023.
\bibitem{example6} F. Scientist, ``Evaluating multi-modal models under adversarial conditions,'' in Proc. Natural Language Processing Conference, 2024, pp. 78--86.
\end{thebibliography}

\end{document}
